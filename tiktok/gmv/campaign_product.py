import requests
import json
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date
from calendar import monthrange
from dotenv import load_dotenv
import os

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- C·∫§U H√åNH ---
ACCESS_TOKEN = os.getenv("TIKTOK_ACCESS_TOKEN")
ADVERTISER_ID = "6967547145545105410"
STORE_ID = "7494600253418473607"
START_DATE = "2025-06-01"
END_DATE = "2025-09-18"
CAMPAIGN_API_URL = "https://business-api.tiktok.com/open_api/v1.3/gmv_max/report/get/"

HEADERS = {
    "Access-Token": ACCESS_TOKEN,
    "Content-Type": "application/json",
}


# ==============================================================================
# PH·∫¶N 1: C√ÅC H√ÄM L·∫§Y D·ªÆ LI·ªÜU S·∫¢N PH·∫®M (T·ª™ SCRIPT PRODUCT)
# ==============================================================================

def get_bc_ids(access_token, max_retries=3, backoff_factor=3):
    """L·∫•y danh s√°ch BC ID v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i."""
    url = "https://business-api.tiktok.com/open_api/v1.3/bc/get/"
    headers = {'Access-Token': access_token}
    print(">> B∆∞·ªõc 1A: ƒêang l·∫•y danh s√°ch BC ID...")
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 0:
                bc_list = data.get("data", {}).get("list", [])
                bc_ids = [bc["bc_info"]["bc_id"] for bc in bc_list if bc.get("bc_info")]
                print(f"   -> ƒê√£ l·∫•y th√†nh c√¥ng {len(bc_ids)} BC ID.")
                return bc_ids
            else:
                print(f"   -> L·ªói API khi l·∫•y BC ID: {data.get('message')}")
        except requests.exceptions.RequestException as e:
            print(f"   -> L·ªói k·∫øt n·ªëi khi l·∫•y BC ID: {e}")
        if attempt < max_retries - 1:
            time.sleep(backoff_factor)
    print("   -> Kh√¥ng th·ªÉ l·∫•y danh s√°ch BC ID sau nhi·ªÅu l·∫ßn th·ª≠.")
    return []

def fetch_all_tiktok_products(bc_id, store_id, access_token):
    """L·∫•y t·∫•t c·∫£ s·∫£n ph·∫©m cho m·ªôt bc_id v√† store_id c·ª• th·ªÉ."""
    base_url = "https://business-api.tiktok.com/open_api/v1.3/store/product/get/"
    headers = {'Access-Token': access_token, 'Content-Type': 'application/json'}
    all_products = []
    current_page = 1
    total_pages = 1
    print(f">> B∆∞·ªõc 1B: Th·ª≠ l·∫•y s·∫£n ph·∫©m v·ªõi BC ID: {bc_id}...")
    while current_page <= total_pages:
        params = {'bc_id': bc_id, 'store_id': store_id, 'page': current_page, 'page_size': 100}
        try:
            response = requests.get(base_url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            if data.get("code") != 0:
                print(f"   -> L·ªói: {data.get('message')}. BC ID n√†y kh√¥ng c√≥ quy·ªÅn.")
                return None # Tr·∫£ v·ªÅ None ƒë·ªÉ b√°o hi·ªáu BC ID kh√¥ng h·ª£p l·ªá
            
            api_data = data.get("data", {})
            products = api_data.get("store_products", [])
            page_info = api_data.get("page_info", {})
            if not products and current_page == 1:
                print(f"   -> Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o.")
                break
            all_products.extend(products)
            if current_page == 1:
                total_pages = page_info.get("total_page", 1)
            print(f"   -> ƒê√£ l·∫•y trang {current_page}/{total_pages}. T·ªïng s·∫£n ph·∫©m hi·ªán t·∫°i: {len(all_products)}")
            current_page += 1
        except requests.exceptions.RequestException as e:
            print(f"   -> ƒê√£ x·∫£y ra l·ªói khi g·ªçi API: {e}")
            return None
    return all_products

def get_product_map(access_token, store_id):
    """
    L·∫•y to√†n b·ªô s·∫£n ph·∫©m v√† chuy·ªÉn th√†nh m·ªôt dictionary ƒë·ªÉ tra c·ª©u nhanh.
    """
    print("\n--- B∆Ø·ªöC 1: L·∫§Y V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU S·∫¢N PH·∫®M ---")
    bc_ids_list = get_bc_ids(access_token)
    if not bc_ids_list:
        return None

    all_products = []
    for bc_id in bc_ids_list:
        products_list = fetch_all_tiktok_products(bc_id, store_id, access_token)
        if products_list is not None:
            print(f"   => TH√ÄNH C√îNG! T√¨m th·∫•y BC ID h·ª£p l·ªá: {bc_id}. ƒê√£ l·∫•y {len(products_list)} s·∫£n ph·∫©m.")
            all_products = products_list
            break
    
    if not all_products:
        print("   -> Kh√¥ng t√¨m th·∫•y BC ID n√†o c√≥ th·ªÉ truy c·∫≠p s·∫£n ph·∫©m c·ªßa store n√†y.")
        return None

    print("\n>> B∆∞·ªõc 1C: T·∫°o b·∫£n ƒë·ªì s·∫£n ph·∫©m ƒë·ªÉ tra c·ª©u nhanh...")
    product_map = {p['item_group_id']: p for p in all_products}
    print(f"   -> ƒê√£ t·∫°o b·∫£n ƒë·ªì cho {len(product_map)} s·∫£n ph·∫©m ƒë·ªôc nh·∫•t.")
    return product_map

# ==============================================================================
# PH·∫¶N 2: C√ÅC H√ÄM L·∫§Y D·ªÆ LI·ªÜU CAMPAIGN (T·ª™ SCRIPT CAMPAIGN)
# ==============================================================================

def chunk_list(data, size):
    for i in range(0, len(data), size):
        yield data[i:i + size]

def generate_monthly_date_chunks(start_date_str, end_date_str):
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
    chunks = []
    cursor_date = date(start_date.year, start_date.month, 1)
    while cursor_date <= end_date:
        _, last_day = monthrange(cursor_date.year, cursor_date.month)
        month_end = date(cursor_date.year, cursor_date.month, last_day)
        chunks.append({
            'start': max(cursor_date, start_date).strftime('%Y-%m-%d'),
            'end': min(month_end, end_date).strftime('%Y-%m-%d')
        })
        next_month = cursor_date.month + 1
        next_year = cursor_date.year
        if next_month > 12: next_month, next_year = 1, next_year + 1
        cursor_date = date(next_year, next_month, 1)
    return chunks

def make_api_request_with_backoff(session, params, max_retries=5, base_delay=3):
    for attempt in range(max_retries):
        try:
            response = session.get(CAMPAIGN_API_URL, params=params, timeout=45)
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 0: return data
            print(f"   [L·ªñI API] {data.get('message')}")
            if "Too many requests" not in data.get("message", ""): return None
        except requests.exceptions.RequestException as e:
            print(f"   [L·ªñI M·∫†NG] (l·∫ßn {attempt + 1}): {e}")
        delay = (base_delay ** attempt) + random.uniform(0, 1)
        time.sleep(delay)
    return None

def fetch_all_pages(session, params):
    all_results, current_page = [], 1
    while True:
        params['page'] = current_page
        data = make_api_request_with_backoff(session, params)
        if not data: break
        page_data = data.get("data", {})
        all_results.extend(page_data.get("list", []))
        total_pages = page_data.get("page_info", {}).get("total_page", 1)
        if current_page >= total_pages: break
        current_page += 1
    return all_results

def get_all_campaigns(session, start_date, end_date):
    params = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id"]),
        "metrics": json.dumps(["campaign_name", "operation_status", "bid_type"]),
        "filtering": json.dumps({"gmv_max_promotion_types": ["PRODUCT"]}), "page_size": 1000,
    }
    all_items = fetch_all_pages(session, params)
    campaigns_map = {}
    if all_items:
        for item in all_items:
            cid = item["dimensions"]["campaign_id"]
            metrics = item["metrics"]
            campaigns_map[cid] = {
                "campaign_name": metrics.get("campaign_name"),
                "operation_status": metrics.get("operation_status"),
                "bid_type": metrics.get("bid_type"),
            }
    return campaigns_map

def fetch_data_for_batch(campaign_batch, campaigns_map, start_date, end_date):
    batch_ids = list(campaign_batch.keys())
    params_perf = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id", "item_group_id", "stat_time_day"]),
        "metrics": json.dumps(["orders", "gross_revenue", "cost", "cost_per_order", "roi"]),
        "filtering": json.dumps({"campaign_ids": batch_ids}), "page_size": 1000,
    }
    with requests.Session() as session:
        session.headers.update(HEADERS)
        perf_list = fetch_all_pages(session, params_perf)
    
    results = {}
    for cid in batch_ids:
        info = campaigns_map.get(cid, {})
        results[cid] = {
            "campaign_id": cid, "campaign_name": info.get("campaign_name"),
            "operation_status": info.get("operation_status"), "bid_type": info.get("bid_type"),
            "performance_data": []
        }
    for record in perf_list:
        cid = record["dimensions"]["campaign_id"]
        if cid in results:
            results[cid]["performance_data"].append(record)
    return list(results.values())


# ==============================================================================
# PH·∫¶N 3: H√ÄM TH·ª∞C THI CH√çNH
# ==============================================================================

if __name__ == "__main__":
    start_time = time.perf_counter()
    if not ACCESS_TOKEN:
        print("L·ªñI: Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng TIKTOK_ACCESS_TOKEN trong file .env")
    else:
        # B∆Ø·ªöC 1: L·∫•y d·ªØ li·ªáu s·∫£n ph·∫©m
        product_map = get_product_map(ACCESS_TOKEN, STORE_ID)

        if product_map:
            # B∆Ø·ªöC 2: L·∫•y d·ªØ li·ªáu campaign
            print("\n--- B∆Ø·ªöC 2: L·∫§Y D·ªÆ LI·ªÜU CAMPAIGN ---")
            date_chunks = generate_monthly_date_chunks(START_DATE, END_DATE)
            all_campaign_results = []

            with requests.Session() as session:
                session.headers.update(HEADERS)
                for chunk in date_chunks:
                    print(f"\n>> X·ª≠ l√Ω chunk: {chunk['start']} to {chunk['end']}")
                    campaigns = get_all_campaigns(session, chunk['start'], chunk['end'])
                    if not campaigns:
                        print("   -> Kh√¥ng c√≥ campaign n√†o trong kho·∫£ng th·ªùi gian n√†y.")
                        continue
                    
                    print(f"   -> T√¨m th·∫•y {len(campaigns)} campaigns. Chia th√†nh c√°c l√¥ ƒë·ªÉ x·ª≠ l√Ω song song...")
                    batches = list(chunk_list(list(campaigns.items()), 20))
                    
                    with ThreadPoolExecutor(max_workers=4) as executor:
                        future_to_batch = {
                            executor.submit(fetch_data_for_batch, dict(batch), campaigns, chunk['start'], chunk['end']): batch
                            for batch in batches
                        }
                        for future in as_completed(future_to_batch):
                            all_campaign_results.extend(future.result())

            # B∆Ø·ªöC 3: G·ªôp d·ªØ li·ªáu s·∫£n ph·∫©m v√†o campaign
            print("\n--- B∆Ø·ªöC 3: G·ªòP D·ªÆ LI·ªÜU S·∫¢N PH·∫®M V√ÄO CAMPAIGN ---")
            enriched_results = []
            for campaign in all_campaign_results:
                if not campaign.get("performance_data"):
                    print(f"   -> B·ªè qua campaign '{campaign['campaign_name']}' v√¨ kh√¥ng c√≥ d·ªØ li·ªáu hi·ªáu su·∫•t.")
                    continue
                
                for perf_record in campaign["performance_data"]:
                    item_id = perf_record.get("dimensions", {}).get("item_group_id")
                    if item_id:
                        # G·∫Øn th√¥ng tin s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng v√†o m·ªói b·∫£n ghi
                        perf_record["product_info"] = product_map.get(item_id, {"title": "Kh√¥ng t√¨m th·∫•y th√¥ng tin"})
                enriched_results.append(campaign)
            print("   -> ƒê√£ g·ªôp d·ªØ li·ªáu th√†nh c√¥ng.")
            
            # B∆Ø·ªöC 4: Xu·∫•t file
            print("\n--- B∆Ø·ªöC 4: L∆ØU K·∫æT QU·∫¢ ---")
            output_filename = "GMV_Campaign_product_detail.json"
            with open(output_filename, "w", encoding="utf-8") as f:
                json.dump(enriched_results, f, ensure_ascii=False, indent=4)
            print(f"   -> ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file '{output_filename}'")
            
            total_cost = 0
            for campaign_result in enriched_results:
                for perf_record in campaign_result.get("performance_data", []):
                    try:
                        cost_value = float(perf_record.get("metrics", {}).get("cost", 0))
                        total_cost += cost_value
                    except (ValueError, TypeError):
                        continue
            print(f"\nüí∞ T·ªïng chi ph√≠ (cost) c·ªßa t·∫•t c·∫£ c√°c campaign ƒë√£ x·ª≠ l√Ω: {total_cost:,.0f} VND")


    end_time = time.perf_counter()
    print(f"\n--- HO√ÄN T·∫§T ---")
    print(f"T·ªïng th·ªùi gian th·ª±c thi: {end_time - start_time:.2f} gi√¢y.")
    # return enriched_results
