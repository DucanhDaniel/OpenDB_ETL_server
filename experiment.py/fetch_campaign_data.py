import requests
import json
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date
from calendar import monthrange
from dotenv import load_dotenv
import os

load_dotenv()

# --- C·∫§U H√åNH ---
ACCESS_TOKEN = os.getenv("TIKTOK_ACCESS_TOKEN") 
ADVERTISER_ID = "6967547145545105410"
STORE_ID = "7494600253418473607"
START_DATE = "2025-09-01"
END_DATE = "2025-09-18"
API_URL = "https://business-api.tiktok.com/open_api/v1.3/gmv_max/report/get/"

HEADERS = {
    "Access-Token": ACCESS_TOKEN,
    "Content-Type": "application/json",
}

def chunk_list(data, size):
    """Chia m·ªôt danh s√°ch th√†nh c√°c danh s√°ch con c√≥ k√≠ch th∆∞·ªõc `size`."""
    for i in range(0, len(data), size):
        yield data[i:i + size]

def generate_monthly_date_chunks(start_date_str, end_date_str):
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
    chunks = []
    cursor_date = date(start_date.year, start_date.month, 1)
    while cursor_date <= end_date:
        _, last_day_of_month = monthrange(cursor_date.year, cursor_date.month)
        month_end_date = date(cursor_date.year, cursor_date.month, last_day_of_month)
        chunk_start = max(cursor_date, start_date)
        chunk_end = min(month_end_date, end_date)
        chunks.append({
            'start': chunk_start.strftime('%Y-%m-%d'),
            'end': chunk_end.strftime('%Y-%m-%d')
        })
        next_month = cursor_date.month + 1
        next_year = cursor_date.year
        if next_month > 12:
            next_month = 1
            next_year += 1
        cursor_date = date(next_year, next_month, 1)
    return chunks

def make_api_request_with_backoff(session, params, max_retries=5, base_delay=3):
    for attempt in range(max_retries):
        try:
            response = session.get(API_URL, params=params, timeout=45)
            print(API_URL, params)
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 0: return data
            if "Too many requests" in data.get("message", ""):
                print(f"  [RATE LIMIT] G·∫∑p l·ªói (l·∫ßn {attempt + 1}/{max_retries})...")
            else:
                print(f"  [L·ªñI API] {data.get('message')}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"  [L·ªñI M·∫†NG] (l·∫ßn {attempt + 1}/{max_retries}): {e}")
        if attempt < max_retries - 1:
            delay = (base_delay ** attempt) + random.uniform(0, 1)
            print(f"  Th·ª≠ l·∫°i sau {delay:.2f} gi√¢y.")
            time.sleep(delay)
    print("  [TH·∫§T B·∫†I] ƒê√£ th·ª≠ l·∫°i t·ªëi ƒëa.")
    return None

def fetch_all_pages(session, params):
    all_results = []
    current_page = 1
    while True:
        params['page'] = current_page
        data = make_api_request_with_backoff(session, params)
        if not data: break
        page_data = data.get("data", {})
        result_list = page_data.get("list", [])
        all_results.extend(result_list)
        page_info = page_data.get("page_info", {})
        total_pages = page_info.get("total_page", 1)
        print(f"  [PH√ÇN TRANG] ƒê√£ l·∫•y trang {current_page}/{total_pages}...")
        if current_page >= total_pages: break
        current_page += 1
        time.sleep(100)
    return all_results

def get_all_campaigns(start_date, end_date):
    print(f"B∆∞·ªõc 1: ƒêang l·∫•y danh s√°ch Campaigns t·ª´ {start_date} ƒë·∫øn {end_date}...")
    params = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id"]), "metrics": json.dumps(["campaign_name"]),
        "filtering": json.dumps({"gmv_max_promotion_types": ["PRODUCT"]}), "page_size": 1000,
    }
    with requests.Session() as session:
        session.headers.update(HEADERS)
        all_campaign_items = fetch_all_pages(session, params)
    if all_campaign_items:
        campaigns = {item["dimensions"]["campaign_id"]: item["metrics"]["campaign_name"] for item in all_campaign_items}
        print(f"==> T√¨m th·∫•y t·ªïng c·ªông {len(campaigns)} campaigns.")
        return campaigns
    return {}

def fetch_data_for_batch(campaign_batch, campaign_name_map, start_date, end_date):
    """
    L·∫•y d·ªØ li·ªáu cho m·ªôt l√¥ c√°c campaign_id c√πng l√∫c.
    """
    batch_ids = [cid for cid in campaign_batch]
    print(f"  [B·∫ÆT ƒê·∫¶U BATCH] X·ª≠ l√Ω {len(batch_ids)} campaigns...")
    
    params_perf = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id", "item_group_id"]),
        "metrics": json.dumps(["cost"]),
        "filtering": json.dumps({"campaign_ids": batch_ids}), 
        "page_size": 1000,
    }

    with requests.Session() as session:
        session.headers.update(HEADERS)
        perf_list = fetch_all_pages(session, params_perf)
        
    # --- Ph√¢n lo·∫°i l·∫°i k·∫øt qu·∫£ cho t·ª´ng campaign trong batch ---
    results_in_batch = {}
    for cid in batch_ids:
        results_in_batch[cid] = {
            "campaign_id": cid,
            "campaign_name": campaign_name_map.get(cid, "N/A"),
            "performance_data": [],
        }

    for record in perf_list:
        cid = record.get("dimensions", {}).get("campaign_id")
        if cid in results_in_batch:
            results_in_batch[cid]["performance_data"].append(record)
            
    print(f"  [OK BATCH] Ho√†n th√†nh x·ª≠ l√Ω {len(batch_ids)} campaigns.")
    return list(results_in_batch.values())

if __name__ == "__main__":
    start_time = time.perf_counter()
    date_chunks = generate_monthly_date_chunks(START_DATE, END_DATE)
    print(f"ƒê√£ chia kho·∫£ng th·ªùi gian th√†nh {len(date_chunks)} chunk.")

    all_final_results = []
    
    for chunk in date_chunks:
        chunk_start = chunk['start']
        chunk_end = chunk['end']
        print(f"\n--- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù CHUNK: {chunk_start} to {chunk_end} ---")
        
        campaigns_map = get_all_campaigns(chunk_start, chunk_end)

        if campaigns_map:
            campaign_ids = list(campaigns_map.keys())
            campaign_batches = list(chunk_list(campaign_ids, 20)) # Chia th√†nh c√°c l√¥ 20 campaign
            print(f"ƒê√£ chia {len(campaign_ids)} campaigns th√†nh {len(campaign_batches)} l√¥.")
            
            max_workers = 4 
            print(f"B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu chi ti·∫øt song song v·ªõi {max_workers} lu·ªìng...")

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_batch = {
                    executor.submit(fetch_data_for_batch, batch, campaigns_map, chunk_start, chunk_end): batch 
                    for batch in campaign_batches
                }
                
                for future in as_completed(future_to_batch):
                    batch_result = future.result() 
                    for result in batch_result:
                        if result.get("performance_data"):
                            all_final_results.append(result)
                        else:
                            print(f"  [B·ªé QUA] Campaign '{result.get('campaign_name')}' v√¨ thi·∫øu d·ªØ li·ªáu.")

    total_cost = 0
    for campaign_result in all_final_results:
        for perf_record in campaign_result.get("performance_data", []):
            try:
                cost_value = float(perf_record.get("metrics", {}).get("cost", 0))
                total_cost += cost_value
            except (ValueError, TypeError):
                continue
    
    print(f"\nüí∞ T·ªïng chi ph√≠ (cost) c·ªßa t·∫•t c·∫£ c√°c campaign ƒë√£ x·ª≠ l√Ω: {total_cost:,.0f} VND")
    print("\n--- HO√ÄN TH√ÄNH TO√ÄN B·ªò ---")
    print(f"ƒê√£ x·ª≠ l√Ω v√† gi·ªØ l·∫°i k·∫øt qu·∫£ t·ª´ {len(all_final_results)} l∆∞·ª£t campaign c√≥ d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß.")
    
    with open("tiktok_results_final_batch.json", "w", encoding="utf-8") as f:
        json.dump(all_final_results, f, ensure_ascii=False, indent=4)
    print("K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c ghi v√†o file tiktok_results_final_batch.json")
    
    end_time = time.perf_counter()
    print(f"\nT·ªïng th·ªùi gian th·ª±c thi: {end_time - start_time:.2f} gi√¢y.")