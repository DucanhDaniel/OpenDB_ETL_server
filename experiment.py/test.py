import requests
import json
import time
import random
from datetime import datetime, date
from calendar import monthrange
from dotenv import load_dotenv
import os

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- C·∫§U H√åNH ---
ACCESS_TOKEN = os.getenv("TIKTOK_ACCESS_TOKEN") 
ADVERTISER_ID = "6967547145545105410"
STORE_ID = "7494600253418473607"
START_DATE = "2025-09-01"
END_DATE = "2025-09-18"
API_URL = "https://business-api.tiktok.com/open_api/v1.3/gmv_max/report/get/"

HEADERS = {
    "Access-Token": ACCESS_TOKEN,
    "Content-Type": "application/json",
}

def chunk_list(data, size):
    """Chia m·ªôt danh s√°ch th√†nh c√°c danh s√°ch con c√≥ k√≠ch th∆∞·ªõc `size`."""
    for i in range(0, len(data), size):
        yield data[i:i + size]

def generate_monthly_date_chunks(start_date_str, end_date_str):
    """Chia m·ªôt kho·∫£ng th·ªùi gian th√†nh c√°c chunk theo th√°ng."""
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
    chunks = []
    cursor_date = date(start_date.year, start_date.month, 1)
    while cursor_date <= end_date:
        _, last_day_of_month = monthrange(cursor_date.year, cursor_date.month)
        month_end_date = date(cursor_date.year, cursor_date.month, last_day_of_month)
        chunk_start = max(cursor_date, start_date)
        chunk_end = min(month_end_date, end_date)
        chunks.append({
            'start': chunk_start.strftime('%Y-%m-%d'),
            'end': chunk_end.strftime('%Y-%m-%d')
        })
        next_month = cursor_date.month + 1
        next_year = cursor_date.year
        if next_month > 12:
            next_month = 1
            next_year += 1
        cursor_date = date(next_year, next_month, 1)
    return chunks

def make_api_request_with_backoff(session, params, max_retries=5, base_delay=3):
    """Th·ª±c hi·ªán g·ªçi API v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i (exponential backoff)."""
    for attempt in range(max_retries):
        try:
            response = session.get(API_URL, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 0: return data
            if "Too many requests" in data.get("message", ""):
                print(f"   [RATE LIMIT] G·∫∑p l·ªói (l·∫ßn {attempt + 1}/{max_retries})...")
            else:
                print(f"   [L·ªñI API] {data.get('message')}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"   [L·ªñI M·∫†NG] (l·∫ßn {attempt + 1}/{max_retries}): {e}")
        if attempt < max_retries - 1:
            delay = (base_delay ** attempt) + random.uniform(0, 1)
            print(f"   Th·ª≠ l·∫°i sau {delay:.2f} gi√¢y.")
            time.sleep(delay)
    print("   [TH·∫§T B·∫†I] ƒê√£ th·ª≠ l·∫°i t·ªëi ƒëa.")
    return None

def fetch_all_pages(session, params):
    """L·∫•y t·∫•t c·∫£ c√°c trang k·∫øt qu·∫£ t·ª´ m·ªôt API endpoint."""
    all_results = []
    current_page = 1
    while True:
        params['page'] = current_page
        data = make_api_request_with_backoff(session, params)
        if not data: break
        page_data = data.get("data", {})
        result_list = page_data.get("list", [])
        all_results.extend(result_list)
        page_info = page_data.get("page_info", {})
        total_pages = page_info.get("total_page", 1)
        if current_page >= total_pages: break
        current_page += 1
        time.sleep(0.5)
    return all_results

def get_all_campaigns(session, start_date, end_date):
    """L·∫•y danh s√°ch t·∫•t c·∫£ campaign ID v√† t√™n trong m·ªôt kho·∫£ng th·ªùi gian."""
    print(f"B∆∞·ªõc 1: ƒêang l·∫•y danh s√°ch Campaigns t·ª´ {start_date} ƒë·∫øn {end_date}...")
    params = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id"]), "metrics": json.dumps(["campaign_name"]),
        "filtering": json.dumps({"gmv_max_promotion_types": ["PRODUCT"]}), "page_size": 1000,
    }
    all_campaign_items = fetch_all_pages(session, params)
    if all_campaign_items:
        campaigns = {item["dimensions"]["campaign_id"]: item["metrics"]["campaign_name"] for item in all_campaign_items}
        print(f"==> T√¨m th·∫•y t·ªïng c·ªông {len(campaigns)} campaigns.")
        return campaigns
    return {}

def fetch_data_for_batch(session, campaign_batch, campaign_name_map, start_date, end_date):
    """L·∫•y d·ªØ li·ªáu hi·ªáu su·∫•t cho m·ªôt l√¥ c√°c campaign_id."""
    batch_ids = [cid for cid in campaign_batch]
    
    params_perf = {
        "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date, "end_date": end_date,
        "dimensions": json.dumps(["campaign_id", "item_group_id"]),
        "metrics": json.dumps(["cost"]),
        "filtering": json.dumps({"campaign_ids": batch_ids}), 
        "page_size": 1000,
    }

    perf_list = fetch_all_pages(session, params_perf)
    
    results_in_batch = {}
    for cid in batch_ids:
        results_in_batch[cid] = {
            "campaign_id": cid,
            "campaign_name": campaign_name_map.get(cid, "N/A"),
            "performance_data": [],
        }

    for record in perf_list:
        cid = record.get("dimensions", {}).get("campaign_id")
        if cid in results_in_batch:
            results_in_batch[cid]["performance_data"].append(record)
            
    return list(results_in_batch.values())

def fetch_item_details(session, campaign_id, item_group_id, start_date, end_date):
    """L·∫•y th√¥ng tin chi ti·∫øt item cho m·ªôt c·∫∑p (campaign_id, item_group_id)."""
    params = {
        "advertiser_id": ADVERTISER_ID,
        "store_ids": json.dumps([STORE_ID]),
        "start_date": start_date,
        "end_date": end_date,
        "dimensions": json.dumps(["item_id"]),
        "metrics": json.dumps([
            "title", "tt_account_name", "tt_account_profile_image_url",
            "tt_account_authorization_type", "shop_content_type"
        ]),
        "filtering": json.dumps({
            "campaign_ids": [campaign_id],
            "item_group_ids": [item_group_id]
        }),
        "page_size": 1000,
    }
    return fetch_all_pages(session, params)

def enrich_results_with_item_details(session, results, start_date, end_date):
    """
    L√†m gi√†u k·∫øt qu·∫£ b·∫±ng c√°ch th√™m th√¥ng tin chi ti·∫øt item m·ªôt c√°ch TU·∫¶N T·ª∞.
    """
    print("\nB∆∞·ªõc 3: B·∫Øt ƒë·∫ßu l√†m gi√†u d·ªØ li·ªáu chi ti·∫øt Item (qu·∫£ng c√°o)...")
    
    tasks = []
    for campaign_result in results:
        for perf_record in campaign_result.get("performance_data", []):
            dims = perf_record.get("dimensions", {})
            cid = dims.get("campaign_id")
            item_group_id = dims.get("item_group_id")
            if cid and item_group_id:
                tasks.append((perf_record, cid, item_group_id))

    if not tasks:
        print("==> Kh√¥ng c√≥ c·∫∑p (campaign, item_group) n√†o ƒë·ªÉ l√†m gi√†u d·ªØ li·ªáu.")
        return results

    print(f"==> Chu·∫©n b·ªã g·ªçi API tu·∫ßn t·ª± cho {len(tasks)} c·∫∑p (campaign, item_group)...")
    
    # V√≤ng l·∫∑p tu·∫ßn t·ª± thay cho ThreadPoolExecutor
    for i, (perf_record, cid, item_group_id) in enumerate(tasks, 1):
        try:
            item_details_list = fetch_item_details(session, cid, item_group_id, start_date, end_date)
            perf_record["item_details"] = item_details_list
            print(f"   [ENRICH] ƒê√£ x·ª≠ l√Ω {i}/{len(tasks)} c·∫∑p...", end='\r')
        except Exception as e:
            print(f"\n   [L·ªñI ENRICH] L·ªói khi x·ª≠ l√Ω c·∫∑p ({cid}, {item_group_id}): {e}")
            perf_record["item_details"] = [] # Ghi nh·∫≠n l·ªói
    
    print(f"\n==> Ho√†n th√†nh l√†m gi√†u d·ªØ li·ªáu chi ti·∫øt item.")
    return results

# --- LU·ªíNG CH√çNH ---

if __name__ == "__main__":
    start_time = time.perf_counter()
    date_chunks = generate_monthly_date_chunks(START_DATE, END_DATE)
    print(f"ƒê√£ chia kho·∫£ng th·ªùi gian th√†nh {len(date_chunks)} chunk.")

    all_final_results = []
    
    with requests.Session() as main_session:
        main_session.headers.update(HEADERS)
    
        for chunk in date_chunks:
            chunk_start = chunk['start']
            chunk_end = chunk['end']
            print(f"\n--- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù CHUNK: {chunk_start} to {chunk_end} ---")
            
            # B∆∞·ªõc 1: L·∫•y t·∫•t c·∫£ campaign trong chunk
            campaigns_map = get_all_campaigns(main_session, chunk_start, chunk_end)

            if campaigns_map:
                campaign_ids = list(campaigns_map.keys())
                campaign_batches = list(chunk_list(campaign_ids, 20))
                
                print(f"B∆∞·ªõc 2: L·∫•y d·ªØ li·ªáu hi·ªáu su·∫•t cho {len(campaign_ids)} campaigns (chia th√†nh {len(campaign_batches)} l√¥).")
                
                chunk_results = []
                
                # V√≤ng l·∫∑p tu·∫ßn t·ª± thay cho ThreadPoolExecutor
                for i, batch in enumerate(campaign_batches, 1):
                    print(f"   ƒêang x·ª≠ l√Ω l√¥ {i}/{len(campaign_batches)}...")
                    batch_result = fetch_data_for_batch(main_session, batch, campaigns_map, chunk_start, chunk_end) 
                    for result in batch_result:
                        if result.get("performance_data"):
                            chunk_results.append(result)

                # B∆∞·ªõc 3: Th√™m b∆∞·ªõc l√†m gi√†u d·ªØ li·ªáu sau khi c√≥ k·∫øt qu·∫£ c·ªßa chunk
                if chunk_results:
                    enriched_chunk_results = enrich_results_with_item_details(main_session, chunk_results, chunk_start, chunk_end)
                    all_final_results.extend(enriched_chunk_results)
                else:
                    print("==> Chunk n√†y kh√¥ng c√≥ d·ªØ li·ªáu hi·ªáu su·∫•t ƒë·ªÉ x·ª≠ l√Ω.")

    # --- T√çNH TO√ÅN V√Ä L∆ØU FILE ---

    if not all_final_results:
        print("\n--- KH√îNG C√ì D·ªÆ LI·ªÜU ---")
        print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ph√π h·ª£p v·ªõi c√°c ti√™u ch√≠.")
    else:
        total_cost = 0
        for campaign_result in all_final_results:
            for perf_record in campaign_result.get("performance_data", []):
                try:
                    cost_value = float(perf_record.get("metrics", {}).get("cost", 0))
                    total_cost += cost_value
                except (ValueError, TypeError):
                    continue
        
        print(f"\nüí∞ T·ªïng chi ph√≠ (cost) c·ªßa t·∫•t c·∫£ c√°c campaign ƒë√£ x·ª≠ l√Ω: {total_cost:,.0f} VND")
        print("\n--- HO√ÄN TH√ÄNH TO√ÄN B·ªò ---")
        print(f"ƒê√£ x·ª≠ l√Ω v√† gi·ªØ l·∫°i k·∫øt qu·∫£ t·ª´ {len(all_final_results)} l∆∞·ª£t campaign c√≥ d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß.")
        
        output_filename = "tiktok_results_ENRICHED_SEQUENTIAL.json"
        with open(output_filename, "w", encoding="utf-8") as f:
            json.dump(all_final_results, f, ensure_ascii=False, indent=4)
        print(f"K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c ghi v√†o file '{output_filename}'")
    
    end_time = time.perf_counter()
    print(f"\nT·ªïng th·ªùi gian th·ª±c thi: {end_time - start_time:.2f} gi√¢y.")