import requests
import json
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date
from calendar import monthrange
from dotenv import load_dotenv
import os

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# --- C·∫§U H√åNH ---
ACCESS_TOKEN = os.getenv("TIKTOK_ACCESS_TOKEN") 
ADVERTISER_ID = "6967547145545105410"
STORE_ID = "7494600253418473607"
START_DATE = "2025-09-01"
END_DATE = "2025-09-18"
API_URL = "https://business-api.tiktok.com/open_api/v1.3/gmv_max/report/get/"

HEADERS = {
    "Access-Token": ACCESS_TOKEN,
    "Content-Type": "application/json",
}

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---

def chunk_list(data, size):
    """Chia m·ªôt danh s√°ch th√†nh c√°c danh s√°ch con c√≥ k√≠ch th∆∞·ªõc `size`."""
    for i in range(0, len(data), size):
        yield data[i:i + size]

def generate_monthly_date_chunks(start_date_str, end_date_str):
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
    chunks = []
    cursor_date = date(start_date.year, start_date.month, 1)
    while cursor_date <= end_date:
        _, last_day_of_month = monthrange(cursor_date.year, cursor_date.month)
        month_end_date = date(cursor_date.year, cursor_date.month, last_day_of_month)
        chunk_start = max(cursor_date, start_date)
        chunk_end = min(month_end_date, end_date)
        chunks.append({
            'start': chunk_start.strftime('%Y-%m-%d'),
            'end': chunk_end.strftime('%Y-%m-%d')
        })
        next_month = cursor_date.month + 1
        next_year = cursor_date.year
        if next_month > 12:
            next_month = 1
            next_year += 1
        cursor_date = date(next_year, next_month, 1)
    return chunks

# --- C√ÅC H√ÄM G·ªåI API V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU ---

def make_api_request_with_backoff(session, params, max_retries=5, base_delay=3):
    """Th·ª±c hi·ªán m·ªôt y√™u c·∫ßu API v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i khi g·∫∑p l·ªói."""
    for attempt in range(max_retries):
        try:
            response = session.get(API_URL, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()
            if data.get("code") == 0: return data
            if "Too many requests" in data.get("message", ""):
                print(f"  [RATE LIMIT] G·∫∑p l·ªói (l·∫ßn {attempt + 1}/{max_retries})...")
            else:
                print(f"  [L·ªñI API] {data.get('message')}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"  [L·ªñI M·∫†NG] (l·∫ßn {attempt + 1}/{max_retries}): {e}")
        if attempt < max_retries - 1:
            delay = (base_delay ** attempt) + random.uniform(0, 1)
            print(f"  Th·ª≠ l·∫°i sau {delay:.2f} gi√¢y.")
            time.sleep(delay)
    print("  [TH·∫§T B·∫†I] ƒê√£ th·ª≠ l·∫°i t·ªëi ƒëa.")
    return None

def fetch_all_pages(session, params):
    """L·∫•y d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c trang c·ªßa m·ªôt y√™u c·∫ßu API."""
    all_results = []
    current_page = 1
    while True:
        params['page'] = current_page
        data = make_api_request_with_backoff(session, params)
        if not data or data.get("code") != 0: break
        
        page_data = data.get("data", {})
        result_list = page_data.get("list", [])
        all_results.extend(result_list)
        
        page_info = page_data.get("page_info", {})
        total_pages = page_info.get("total_page", 1)
        print(f"  [PH√ÇN TRANG] ƒê√£ l·∫•y trang {current_page}/{total_pages}...")
        
        if current_page >= total_pages: break
        current_page += 1
        time.sleep(1.2)
    return all_results

def enrich_with_creative_details(product_perf_list, creative_api_results):
    """L√†m gi√†u d·ªØ li·ªáu s·∫£n ph·∫©m b·∫±ng c√°ch th√™m chi ti·∫øt creative."""
    creative_details_map = {}
    for creative_result in creative_api_results:
        dimensions = creative_result.get("dimensions", {})
        product_id = dimensions.get("item_group_id")
        if not product_id: continue
        
        creative_info = {"item_id": dimensions.get("item_id"), "metrics": creative_result.get("metrics", {})}
        
        if product_id not in creative_details_map:
            creative_details_map[product_id] = []
        creative_details_map[product_id].append(creative_info)

    for product_perf in product_perf_list:
        current_product_id = product_perf.get("dimensions", {}).get("item_group_id")
        enriched_data = creative_details_map.get(current_product_id, [])
        product_perf["creative_details"] = enriched_data
        
    return product_perf_list

def filter_empty_creatives(enriched_campaign_data):
    """L·ªçc b·ªè c√°c creative kh√¥ng c√≥ b·∫•t k·ª≥ ch·ªâ s·ªë hi·ªáu su·∫•t n√†o."""
    print("B·∫Øt ƒë·∫ßu l·ªçc c√°c creative kh√¥ng c√≥ hi·ªáu su·∫•t...")
    ZERO_METRICS = {
        "cost", "orders", "gross_revenue", "product_clicks", 
        "product_impressions", "ad_video_view_rate_2s"
    }
    
    for campaign in enriched_campaign_data:
        for product in campaign.get("performance_data", []):
            if "creative_details" in product:
                filtered_creatives = []
                for creative in product["creative_details"]:
                    metrics = creative.get("metrics", {})
                    is_all_zero = True
                    for key, value in metrics.items():
                        if key in ZERO_METRICS and float(value) != 0:
                            is_all_zero = False
                            break
                    if not is_all_zero:
                        filtered_creatives.append(creative)
                product["creative_details"] = filtered_creatives
    return enriched_campaign_data


def process_campaign_batch(campaign_batch, start_date, end_date):
    """X·ª≠ l√Ω m·ªôt l√¥ campaigns (v√≠ d·ª•: 2 campaign m·ªôt l√∫c)."""
    batch_ids = [c[0] for c in campaign_batch]
    batch_names = [c[1] for c in campaign_batch]
    print(f"  [B·∫ÆT ƒê·∫¶U BATCH] X·ª≠ l√Ω {len(batch_ids)} campaigns: {', '.join(batch_names)}")
    
    batch_results = {
        cid: {"campaign_id": cid, "campaign_name": cname, "performance_data": []}
        for cid, cname in campaign_batch
    }

    with requests.Session() as session:
        session.headers.update(HEADERS)

        # 1. L·∫•y t·∫•t c·∫£ s·∫£n ph·∫©m cho c·∫£ l√¥ campaign n√†y
        params_product = {
            "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
            "start_date": start_date, "end_date": end_date,
            "dimensions": json.dumps(["campaign_id", "item_group_id"]),
            "metrics": json.dumps(["cost", "orders", "gross_revenue"]),
            "filtering": json.dumps({"campaign_ids": batch_ids}),
            "page_size": 1000,
        }
        product_perf_list = fetch_all_pages(session, params_product)

        if not product_perf_list:
            print(f"  [K·∫æT TH√öC BATCH] L√¥ campaigns kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m.")
            return list(batch_results.values())

        # 2. L·∫•y chi ti·∫øt creative cho t·∫•t c·∫£ s·∫£n ph·∫©m trong l√¥
        product_ids = list(set([p["dimensions"]["item_group_id"] for p in product_perf_list]))
        product_id_chunks = list(chunk_list(product_ids, 20)) # Chia l√¥ 20 s·∫£n ph·∫©m/l·∫ßn
        
        all_creative_results = []
        print(f"  T√¨m th·∫•y {len(product_ids)} s·∫£n ph·∫©m duy nh·∫•t, chia th√†nh {len(product_id_chunks)} l√¥ ƒë·ªÉ l·∫•y creative.")
        for p_chunk in product_id_chunks:
            params_creative = {
                "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
                "start_date": start_date, "end_date": end_date,
                "dimensions": json.dumps(["campaign_id", "item_group_id", "item_id"]),
                "metrics": json.dumps(["cost","orders","cost_per_order","gross_revenue","roi","product_impressions","product_clicks","product_click_rate","ad_conversion_rate","creative_delivery_status","ad_video_view_rate_2s","ad_video_view_rate_6s","ad_video_view_rate_p25","ad_video_view_rate_p50","ad_video_view_rate_p75","ad_video_view_rate_p100"]),
                "filtering": json.dumps({"campaign_ids": batch_ids, "item_group_ids": p_chunk}),
                "page_size": 1000,
            }
            creative_results = fetch_all_pages(session, params_creative)
            all_creative_results.extend(creative_results)
            time.sleep(1.2)

        # 3. L√†m gi√†u v√† ph√¢n lo·∫°i l·∫°i k·∫øt qu·∫£ v√†o ƒë√∫ng campaign
        enriched_product_list = enrich_with_creative_details(product_perf_list, all_creative_results)
        
        for product_record in enriched_product_list:
            cid = product_record.get("dimensions", {}).get("campaign_id")
            if cid in batch_results:
                batch_results[cid]["performance_data"].append(product_record)

    print(f"  [HO√ÄN TH√ÄNH BATCH] ƒê√£ x·ª≠ l√Ω xong l√¥: {', '.join(batch_names)}")
    return list(batch_results.values())

# --- H√ÄM CH√çNH ƒê·ªÇ CH·∫†Y ---

if __name__ == "__main__":
    start_time = time.perf_counter()
    date_chunks = generate_monthly_date_chunks(START_DATE, END_DATE)
    print(f"ƒê√£ chia kho·∫£ng th·ªùi gian th√†nh {len(date_chunks)} chunk.")

    all_enriched_results = []
    
    for chunk in date_chunks:
        chunk_start, chunk_end = chunk['start'], chunk['end']
        print(f"\n--- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù CHUNK: {chunk_start} to {chunk_end} ---")
        
        campaigns_map = {}
        with requests.Session() as session:
            session.headers.update(HEADERS)
            params = {
                "advertiser_id": ADVERTISER_ID, "store_ids": json.dumps([STORE_ID]),
                "start_date": chunk_start, "end_date": chunk_end,
                "dimensions": json.dumps(["campaign_id"]), "metrics": json.dumps(["campaign_name"]),
                "filtering": json.dumps({"gmv_max_promotion_types": ["PRODUCT"]}), "page_size": 1000,
            }
            all_campaign_items = fetch_all_pages(session, params)
            if all_campaign_items:
                campaigns_map = {item["dimensions"]["campaign_id"]: item["metrics"]["campaign_name"] for item in all_campaign_items}
        
        print(f"==> T√¨m th·∫•y {len(campaigns_map)} campaigns trong chunk n√†y.")

        if campaigns_map:
            campaign_list = list(campaigns_map.items())
            campaign_batches = list(chunk_list(campaign_list, 5)) # Chia th√†nh c√°c l√¥ 5 campaign
            
            max_workers = 1
            print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(campaign_batches)} l√¥ song song v·ªõi {max_workers} lu·ªìng...")
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_batch = {
                    executor.submit(process_campaign_batch, batch, chunk_start, chunk_end): batch
                    for batch in campaign_batches
                }
                
                for future in as_completed(future_to_batch):
                    try:
                        batch_result = future.result()
                        for campaign_result in batch_result:
                             if campaign_result and campaign_result.get("performance_data"):
                                all_enriched_results.append(campaign_result)
                    except Exception as exc:
                        print(f"  [L·ªñI LU·ªíNG] L√¥ {future_to_batch[future]} t·∫°o ra l·ªói: {exc}")

    # L·ªçc c√°c creative kh√¥ng c√≥ hi·ªáu su·∫•t
    final_filtered_results = filter_empty_creatives(all_enriched_results)

    # T√≠nh t·ªïng cost c·ªßa c√°c creative C√íN L·∫†I sau khi l·ªçc
    total_creative_cost = 0
    for campaign in final_filtered_results:
        for product in campaign.get("performance_data", []):
            for creative in product.get("creative_details", []):
                try:
                    cost_value = float(creative.get("metrics", {}).get("cost", 0))
                    total_creative_cost += cost_value
                except (ValueError, TypeError):
                    continue
    
    # Ghi k·∫øt qu·∫£ cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l·ªçc ra file
    output_filename = "tiktok_final_results.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(final_filtered_results, f, ensure_ascii=False, indent=4)
    
    print("\n--- HO√ÄN TH√ÄNH TO√ÄN B·ªò ---")
    print(f"ƒê√£ x·ª≠ l√Ω v√† l∆∞u k·∫øt qu·∫£ c·ªßa {len(final_filtered_results)} campaigns v√†o file '{output_filename}'")
    print(f"üí∞ T·ªïng chi ph√≠ (cost) c·ªßa c√°c creatives C√ì HI·ªÜU SU·∫§T: {total_creative_cost:,.0f} VND")
    
    end_time = time.perf_counter()
    print(f"\nT·ªïng th·ªùi gian th·ª±c thi: {end_time - start_time:.2f} gi√¢y.")